{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Artificial Intelligence - Supervised Learning lab Session Part 1a\n",
    "--\n",
    "At the end of this session, you will be able to : \n",
    "- Perform basic supervised learning tasks using sklearn.\n",
    "- Apply supervised learning on PyRat datasets to predict the winner from the start configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tqdm package is useful to visualize progress with long computations. \n",
    "# Install it using pip. \n",
    "import tqdm\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Basics of machine learning using sklearn\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn is a very powerful package that implements most machine learning methods. sklearn also includes cross-validation procedures in order to prevent overfitting, many useful metrics and data manipulation techniques that enables very careful experimentations with machine learning. It is also very straightforward to use. We will introduce a few basic concepts of sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is very easy to simulate data with sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function make_blobs to generate clouds of points with $d=2$, and visualize them using the function scatter from matplotlib.pyplot. You can generate as many samples as you want.You can generate several clouds of points using the argument centers. We recommend using random_state=0 so that your results are from the same distribution as our tests.\n",
    "\n",
    "Vocabulary : n_samples is the number of generated samples, n_features is $d$ (number of dimensions), centers is the number of classes. \n",
    "\n",
    "Hint : you can use the output \"y\" as an argument for the color argument (\"c\") of the scatter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED - Generate blobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED - Plot.\n",
    "### Don't forget to import pyplot and use %matplotlib inline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use other arguments from make_blobs in order to change the variance of the blobs, or the coordinates of their center. You can also experiment on higher dimension, although it becomes difficult to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn has many other data generators, as well as ways to load standard datasets of various sizes. Check them out here: \n",
    "\n",
    "http://scikit-learn.org/stable/datasets/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated a simple dataset, let's try to do a basic supervised learning approach. \n",
    "\n",
    "First, in order to mesure the ability of the model to generalize, we have to split the dataset into a training set and a test set. The test set is the part of the dataset that the model will not see during the training and will be used as a proxy for your \"real world\" examples.\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*u03UsvBGwkYD4E7BObpcaw.png\"></center>\n",
    "<center><small>Image taken from https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, you can use the train_test_split function to split datasets.\n",
    "\n",
    "Try to split the dataset you previously generated (the blobs) into x_train, x_test, y_train, y_test, with 80% in x_train and 20% in x_test. Set random_state = 0 so that the function always returns the same split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CELL TO BE COMPLETED \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes of the generated vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a K-Nearest Neighbor classifier to test whether we can classify this dataset. Create a <b>classifier</b>, train it using your <b> training set </b> and evaluate it by its <b>accuracy</b> on both <b>the train and test sets</b>. \n",
    "\n",
    "In K-Nearest Neighbor classification (also known as KNN), when you want to predict the class of an object, you look at the K (an hyperparameter) nearest examples from the training (using a distance metric, in our case the euclidean distance). This object is then classified by a majority vote among its neighbors. In other words, the class of the object is the most common class among its neighbours.\n",
    "\n",
    "To use a Nearest Neighbor with sklearn, you have to use the class [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).\n",
    "\n",
    "The sklearn API is consistent. This means that for almost every method they propose you can train it using [object.fit](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.fit), you can use it to make prediction with [object.predict](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict) and finally verify the <b>accuracy</b> of the method using [object.score](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED - Train the classifier and get the accuracy in both sets.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 1\n",
    "classifier = KNeighborsClassifier(n_neighbors=k, n_jobs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your classifier should have a train accuracy of 1, while the test accuracy should be high but not perfect.\n",
    "\n",
    "This is caused by the bias-variance trade-off. The 1-NN classifier always has a bias of 0 (it perfectly classifies the training set) but it has a high variance given that having one more example in the training set can completely change a decision.\n",
    "\n",
    "Try to avoid having such a high variance, test different values of k and plot the accuracies given the different values of the hyperparameter k. \n",
    "\n",
    "If you have time, we advise you to do the same analysis but varying the train/test split size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL TO BE COMPLETED - Train networks with varying k.\n",
    "train_acc_p = list()\n",
    "test_acc_p = list()  # list storing the test set accuracies\n",
    "test_ks = range(1,25)  # list containing values of k to be tested\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your classifier is trained, and bias-variance analysed, it is time to look at other metrics based on your results. It is important to remember that accuracy is a key metric, but it is not the <b> only </b> metric you should be focusing on.\n",
    "\n",
    "Print a [classification report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) and a [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) for both training and test sets.\n",
    "\n",
    "In the classification report, you are going to see 3 new metrics. They are really important because the accuracy does not show a complete portrait of your results.\n",
    "\n",
    "* Precision: Percentage of correctly classified examples with respect to all retrieved examples\n",
    "* Recall: Percentage of correctly classified examples with respect to all examples belonging to a given class\n",
    "* F1 Score: Harmonic mean from precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED - Generate the report and confusion matrix for the test set.\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you are going to plot the decision boundaries of our model. Use the function plot_boundaries given below. You can only do this if the tensor representing your data is two dimensional.\n",
    "\n",
    "This function will test our model with values ranging from the smallest x to the highest x and from the lowest y to the highest y, each varying by $h$ and plot it nicely. [Link to the original implementation.](http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "\n",
    "def plot_boundaries(classifier,X,Y,h=0.2):\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    x0, x1 = np.meshgrid(np.arange(x0_min, x0_max,h),\n",
    "                         np.arange(x1_min, x1_max,h))\n",
    "    dataset = np.c_[x0.ravel(),x1.ravel()]\n",
    "    Z = classifier.predict(dataset)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(x0.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(x0, x1, Z)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(x0.min(), x0.max())\n",
    "    plt.ylim(x1.min(), x1.max())\n",
    "plot_boundaries(classifier,x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the winner in PyRat based on the initial cheese configuration\n",
    "--\n",
    "\n",
    "Use the code from lab1 in order to generate a PyRat dataset (X,y) of initial cheese configuration X each corresponding to a label of winner in y. \n",
    "\n",
    "The goal of the next part is to perform supervised learning on this dataset using a KNN classifier, as done above. Use the same metrics to estimate the performance of the classifier. \n",
    "\n",
    "We suggest that you start with a rather small maze, as the problem gets really high to solve in high dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"../../session1/lab/\")\n",
    "#to avoid conflict between j notebook and pyrat arguments\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "import pyrat\n",
    "import simulate_matches_2_opponents as simulations\n",
    "import AI.greedy as greedy_player\n",
    "import AI.random as random_player\n",
    "\n",
    "program_1 = greedy_player \n",
    "program_2 = greedy_player # You may want to test with the random player.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for Lab1, you can run several games with the following function, by specifying the number of desired games with `nb_games` and the maze dimensions and number of cheeses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_games = 10\n",
    "\n",
    "nb_cheese = 4\n",
    "maze_width = 5\n",
    "maze_height = 7\n",
    "\n",
    "cheeses_10_games, winners_10_games = simulations.run_several_games(program_1, program_2, nb_cheese = nb_cheese,\n",
    "                                                                  maze_width = maze_width, maze_height = maze_height, \n",
    "                                                                  nb_games = nb_games, gui = False)\n",
    "\n",
    "#print(f\"Cheeses matrix: {cheeses_10_games}\")\n",
    "print(f\"Winners: {winners_10_games}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `simulations` function to generate a dataset with the desired nb_cheese and maze dimensions and do more tests. However, as a start, you can also use the datasets we generated for you in Lab1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED \n",
    "# Load maze configuration and winners data from Lab1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED \n",
    "# split dataset in train and test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED \n",
    "# train a KNN classifier and test it for different number of neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED \n",
    "#  Generate the report and confusion matrix for the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now explore the performances of the KNN classifier with other maze dimentions and number of cheeses.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
